function [net,addAfter] = AddConvBlock(net, addAfter, name, ksize, depth, varargin)
% Helper function to add a Convolutional + BatchNorm + ReLU
% sequence to the network.
  args.downsample = false;
  args.bias = false ;
  args.order = {'conv','bn','relu'};
  args.isPad = true;
  args.leak = 0;
  cudnnWorkspaceLimit = 1024*1024*1204 ; % 1GB
  args = vl_argparse(args, varargin) ;
  if length(ksize) == 1, ksize(2) = ksize(1); end
  if args.downsample, stride = 2 ; else stride = 1 ; end
  if args.bias, pars = {[name '_f'], [name '_b']} ; else pars = {[name '_f']} ; end
  
  for iO = 1:length(args.order)
      switch args.order{iO}
          case 'conv'
              net.addLayer([name  '_conv'], ...
                  dagnn.Conv('size', [ksize(1) ksize(2) addAfter.depth depth], ...
                  'stride', stride, ....
                  'pad', (ksize - 1) / 2 * args.isPad, ...
                  'hasBias', args.bias, ...
                  'opts', {'cudnnworkspacelimit', cudnnWorkspaceLimit}), ...
                  addAfter.var, ...
                  [name '_conv'], ...
                  pars) ;
              addAfter.depth = depth ;
              addAfter.var = [name '_conv'] ;
          case 'bn'
              net.addLayer([name '_bn'], ...
                  dagnn.BatchNorm('numChannels', addAfter.depth), ...
                  addAfter.var, ...
                  [name '_bn'], ...
                  {[name '_bn_w'], [name '_bn_b'], [name '_bn_m']}) ;
              addAfter.depth = addAfter.depth ;
              addAfter.var = [name '_bn'] ;
          case 'relu'
              net.addLayer([name '_relu'] , ...
                  dagnn.ReLU('leak',args.leak), ...
                  addAfter.var, ...
                  [name '_relu']) ;
              addAfter.depth = addAfter.depth ;
              addAfter.var = [name '_relu'] ;
      end
  end
end